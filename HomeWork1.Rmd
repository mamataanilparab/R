---
title:  "**Homework 1**"
author: "*Mamata Anil Parab*"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
library(MVA)
library(norm)
library(scatterplot3d)
library(KernSmooth)
library(ResourceSelection)
library(MASS)
```


### Problem 1: Answer the following questions.

#### a. How many rows are in an n × 3 matrix? How many columns? Choose a number for n and give an example in R.\newline \newline

Here we consider **n=3**
```{r}
mat= matrix(seq(1,9), byrow=T, nrow=3)
mat
```

Number of rows: 
```{r}
nrow(mat)
```

Number of columns:
```{r}
ncol(mat)
```

OR we can also use **dim** function to see dimensions of matrix.
The first number of dimension indicates number of rows whereas second number indicates 
number of columns

```{r}
dim(mat)
```
So, in **3x3** matrix, 3 columns and 3 rows are present.\newline

#### b. Give an example of a 3 × 2 matrix in R.

The **3x2** matrix is:


```{r}
mat2= matrix(c(1, 2, 8, 9, 5, 6), byrow=T, nrow=3)
mat2
dim(mat2)
```

### Problem 2: You can use R to help you answer the following questions, but you must articulate your explanations in written form. For each of the following three expressions answer the following: (1) Is the expression solvable? (2) If yes, provide a solution; if not, explain why it is not solvable.\newline \newline

#### a.
$$
  \begin{bmatrix} 
2  & -1 \\ 
1 & 2 \\ 
\end{bmatrix}
*
  \begin{bmatrix} 
1 & 1 & 2 \\ 
2 & 0 & 1 \\ 
\end{bmatrix}
$$

In general, matrix multiplication **AxB** of two matrices **A** and **B** is possible if the number of columns of **A** is equal to the number of rows of **B**.


```{r}
A= matrix(c(2,-1,1,2), byrow=T, nrow=2)
A
```
```{r}
B= matrix(c(1,1,2,2,0,1), byrow=T, nrow=2)
B
```
The dimension of A and B are:
  
  ```{r}
dim(A)
```
```{r}
dim(B)
```

```{r}
ncol(A)== nrow(B)
```

Here number of columns of A is equal to number of rows of B. So the given expression is **solvable**.

```{r}
A %*% B
```

#### b.
$$
  \begin{bmatrix} 
1  & 0 \\ 
2 & 1 \\
-2 & 1\\
\end{bmatrix}
*
  \begin{bmatrix} 
1 & 1 & 2 \\ 
2 & 0 & 1 \\ 
\end{bmatrix}
$$

In general, matrix multiplication **AxB** of two matrices **A** and **B** is possible if the number of columns of **A** is equal to the number of rows of **B**.

```{r}
A= matrix(c(1,0,2,1,-2,1), byrow=T, nrow=3)
A
```
```{r}
B= matrix(c(1,1,2,2,0,1), byrow=T, nrow=2)
B
```
The dimension of A and B are:
  
  ```{r}
dim(A)
```
```{r}
dim(B)
```


```{r}
ncol(A)== nrow(B)
```

Here number of columns of A is equal to number of rows of B.So the given expression is **solvable**.

```{r}
A %*% B
```

#### c.
$$
  \begin{bmatrix} 
1  & 0 \\ 
2 & 1 \\
-2 & 1\\
\end{bmatrix}
*
  \begin{bmatrix} 
1 \\ 
-1\\
0\\
\end{bmatrix}
$$
  
  
In general, matrix multiplication **AxB** of two matrices **A** and **B** is possible if the number of columns of **A** is equal to the number of rows of **B**.

```{r}
A= matrix(c(1,0,2,1,-2,1), byrow=T, nrow=3)
A
```
```{r}
B= matrix(c(1,-1,0), ncol=1)
B
```
The dimension of A and B are:
  
  ```{r}
dim(A)
```
```{r}
dim(B)
```


```{r}
ncol(A)== nrow(B)
```
Here number of columns of A is not equal to number of rows of B. So the given expression is **not solvable**.


### Problem 3: You can use R to help you answer the following questions, but you must articulate your explanations in written form. For each of the following three expressions answer the following: (1) Is the expression solvable? (2) If yes, provide a solution; if not, explain why it is not solvable.

#### a.
$A$ * $A$^-1^ 
  where $A$=
  
  $$
  \begin{bmatrix} 
2 & 1 \\ 
1 & 2\\
\end{bmatrix}
$$

To solve this question, we first calculate **determinant** of matrix. If the determinant of matrix is zero then matrix will not have inverse. Such a matrix is called as **Singular** matrix.

```{r}
A= matrix(c(2,1,1,2), byrow=T, nrow=2)
A
```

```{r}
det(A)
```
As determinant of A=3 **(det(A) $\neq$ 0)**, the given expression is **solvable**.\newline

We can find an inverse as:
  
  ```{r}
solve(A)
```

**$A$ x $A$^-1^** can be calculated as:
  
  ```{r}
A %*% solve(A)
```
#### b. 

A * $A$^-1^ 
  where A=
  
  $$
  \begin{bmatrix} 
2 & 0 \\ 
1 & 0\\
\end{bmatrix}
$$

To solve this question, we first calculate **determinant** of matrix. If the determinant of matrix is zero then it will not have inverse. Such a matrix is called as **Singular** matrix.

```{r}
A= matrix(c(2,0,1,0), byrow=T, nrow=2)
A
```

```{r}
det(A)
```

As determinant of A=0 **(det(A) = 0)**, the given expression is **not solvable**.\newline

### Problem 4: Download women’s track record dataset as well as the men’s track record dataset. These datasets hold data on men’s and women’s records for various races in various countries. Note that the columns of the two datasets do not match exactly. \newline


```{r}
women <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/womens_track.csv")
head(women)
```


```{r}
men <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/mens_track.csv")
head(men)
```


#### a. Report the covariance and correlation matrix among the women’s records. Which variables are the most correlated? Why?\newline\newline

Covariance is similar to correlation, except the data are not normalized when the covariance is computed. As a result, the covariance is represented in data-dependent units rather than being converted to a standardized scale of -1 to 1.

```{r}
round(cov(women[seq(1,7)]),6)
```

```{r}
round(cor(women[seq(1,7)]),6)
```


Among all the variables, the correlation coefficient between **m1500** and **m3000** is high i.e. **0.9691690**, indicating a positive, high correlation (dependency) between two records (m1500 and m3000).
Similarly, the covariance between **marathon** and **m400** is high i.e. **57.492462**, indicating a positive and high dependency between two records (marathon and m400)


#### b. Report the covariance and correlation matrix among the men’s records. Which variables are the most correlated? Why? \newline \newline 

Covariance is similar to correlation, except the data are not normalized when the covariance is computed. As a result, the covariance is represented in data-dependent units rather than being converted to a standardized scale of -1 to 1.  \newline\newline
  
 ```{r}
round(cov(men[seq(1,8)]),5)
```

```{r}
round(cor(men[seq(1,8)]),5)
```

Among all the variables, the correlation coefficient between **m3000** and **mystery** is high i.e. **0.97464**,indicating a positive, high correlation (dependency) between two records.
Similarly the covariance between **mystery** and **marathon** is high i.e. **15.73218**, indicating a positive and high dependency between two records


#### c. Find the Euclidean distance matrix for the first five rows of the women’s data combined with the first five rows of the men’s data for columns which appear in both datasets. Present your matrix to 2 decimal places.\newline\newline

First five rows of women data are:
  ```{r}
w_data= women[seq(1,5),seq(1,7)]
w_data
```
```{r}
m_data= men[seq(1,5), c(seq(1,6),8)]
m_data
```
```{r}
combined_data= rbind(w_data, m_data)
combined_data
```

It is useful to standardize the data before calculating the distance, to avoid scale discrepancies.

```{r}
scale_cdata= scale(combined_data)
round(scale_cdata,5)
```

**Euclidean distance**
```{r}
dist_s = dist(scale_cdata, method = "euclidean")
round((dist_s),2)
```

#### d. Given your solution to part (c), which two entries are the most similar? Which are the most dissimilar? Explain how you determined this.\newline\newline

In the euclidean distance matrix, similarity and dissimilarity depends on the distance between two points or entities.

If the distance between two entities is less then they are more likely to be similar or more likely possess similar characteristics.
But if the distance between two entities is higher, then they are more likely to be dissimilar.

As shown in (c) the euclidean distance between the entities 8 and 6 is **0.47** which is the smallest and indicating two entities are most similar in characteristics. Similarly, the euclidean distance between entities 1 and 7 is **6.35** which is largest and indicating two points are most dissimilar in characteristics.

### Problem 5: Answer the following questions.

#### a. Convert the following covariance matrix into the corresponding correlation matrix using R

$$
  Cov=
  \begin{bmatrix} 
3.8778 & 2.8110 & 3.1480 & 3.5062 \\
2.8110 & 2.1210 & 2.2669 & 2.5690\\
3.1480 & 2.2669 & 2.6550 & 2.8341\\
3.5062 & 2.5690 & 2.8341 & 3.2352\\
\end{bmatrix}
$$

The covariance matrix cov is:
  
```{r}
Cov= matrix(c(3.8778, 2.8110, 3.1480, 3.5062,
              2.8110, 2.1210, 2.2669, 2.5690,
              3.1480, 2.2669, 2.6550, 2.8341,
              3.5062, 2.5690, 2.8341, 3.2352), byrow=T, nrow=4)

Cov
```

The corresponding correlation matrix is:

```{r}
cov2cor(Cov)
```

#### \newline \newline \newline \newline b. Show how to find the correlation between the first two variables based on the covariance matrix using simple algebra.\newline

The correlation of first two variables is using simple algebra:
```{r}
Cov[1,2]/(sqrt(Cov[1,1])*sqrt(Cov[2,2]))
```
  
### Problem 6: Use the TTU graduate student exit survey data

```{r}
grad <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/pgs.csv")
```

##### a. Make a scatterplot of two variables: the score of faculty teaching “FacTeaching” and faculty knowledge “FacKnowledge.” Consider using the jitter() function in this analysis, and explain why it may be useful.\newline \newline

jittering introduces random noise to data, which may be useful for plotting data in a scatterplot. We can gain a clearer understanding of the real underlying relationship between two variables in a dataset by utilizing the jitter function.

```{r}
plot(jitter(grad$FacTeaching), jitter(grad$FacKnowledge), col = 'steelblue',
     ylab = "Faculty Knowledge Score",
     xlab = "Faculty Teaching Score")
```



#### b. Create a new data frame for three variables: “FacTeaching”, “FacKnowledge”, and “Housing”.\newline\newline

We will create subset of **grad** data as: 

```{r}
subcol =c("FacTeaching", "FacKnowledge", "Housing")
subgrad= grad[subcol]
head(subgrad)
```


#### \newline \newline c. Find a correlation matrix of the data in part (b). If there are missing values in your data, report how many missing values there are and where they appear. Then estimate the correlation matrix via:\newline
i. Complete-case analysis \newline
ii. Available-case analysis \newline
iii. Maximum likelihood estimation \newline

Is there a noticeable difference among the three methods? Which method do you suggest?
  
  ```{r}
cor(subgrad)
```

```{r}
sum(is.na(subgrad))
```
There are **350** missing values in **subgrad**. 

The rows where missing values are present can be determine by: 
  ```{r}
subgrad_na = subgrad[!complete.cases(subgrad),] # OR subgrad[is.na(subgrad),]
head(subgrad_na, 20) #Here we are showing only first 20 rows
```

***1) Complete Case Analysis***

```{r}
cor(subgrad, use = "complete.obs")
```


***2) Available-case analysis***

```{r}
cor(subgrad, use = "pairwise.complete.obs")
```


***3) Maximum likelihood estimation***

```{r}
library(norm)
s <- prelim.norm(as.matrix(subgrad))
x <- em.norm(s)
getparam.norm(s,x,corr=T)
```

```{r}
getparam.norm(s,x,corr=F)
```

The output for **Complete Case Analysis**, **Available-case analysis** and **Maximum likelihood estimation** are almost equal.
According to me, **Maximum likelihood estimation** is the most appropriate method.
In **Maximum likelihood estimation**, the likelihood is computed separately for complete case on some variables and those with complete data on all variables. These two likelihoods are then maximized together to find the estimates.

### Problem 7: Consider the fish dataset. This data describe a set of measurements made on fish caught in a single lake in Finland. Use the bivariate boxplot on the pairs of variables (weight, height3) and (weight,hgtpct) to identify any outliers. Calculate the correlation between each pair of variables using all complete cases of data and the data with any identified outliers removed. Comment on the results.

Dowloading the dataset as:

```{r}
fish <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/fish.csv")
head(fish)
```

The bivariate boxplot for the pairs of variables (weight, height3) is:

```{r, fig.height=3.5}
fish1 = na.omit(fish[, c('weight', 'length3')])
bvbox(fish1)

```

The bivariate boxplot for the pairs of variables (weight, hgtpct) is:

```{r, fig.height=3.5}
fish2 = na.omit(fish[, c('weight', 'hgtpct')])
bvbox(fish2)
```

***\newline \newline The correlation between weight and length3***

```{r}
w1 = fish1[fish$weight<1100, ]
cor(w1[complete.cases(w1),])
```

***The correlation between weight and hgtpct***
```{r}
w2 = fish2[fish$weight<1500, ]
cor(w2[complete.cases(w2),])
```

### Problem 8: The Swiss banknote dataset contains measurements on 200 Swiss banknotes: 100 genuine and 100 counterfeit. The variables are the status of the “note”, length of bill, width of left edge, width of right edge, bottom margin width, and top margin width. All measurements are in millimeters. Read the data and pick the variables: “note”, “top_margin” and “diag_length”.\newline
Reading the dataset
```{r}
swiss <- read.csv("https://raw.githubusercontent.com/EricBrownTTU/ISQS6350/main/swiss.csv")
head(swiss)
```

Subset of swiss dataset is:

```{r}
subswiss= swiss[, c("note", "top_margin", "diag_length")]
head(subswiss)
```

#### \newline a. Construct separate univariate kernel estimates (using the Gaussian kernel) of the distributions of the two continuous variables. Experiment with bandwidths to get appropriate graphs. \newline \newline 
***Univariate Gaussian kernel estimation for top_margin for different bandwidths***

```{r, figures-side, fig.show="hold", out.width="50%"}
plot(density(subswiss[,2], kernel = "gaussian", bw = 0.1))
plot(density(subswiss[,2], kernel = "gaussian", bw = 0.3))
plot(density(subswiss[,2], kernel = "gaussian", bw = 0.5))
plot(density(subswiss[,2], kernel = "gaussian", bw = 3))
```


***\newline \newline \newline \newline \newline\newline \newline\newline \newline\newline \newline \newline\newline\newline\newline \newline\newline Univariate Gaussian kernel estimation for diag_length for different bandwidths***

```{r, figures-side2, fig.show="hold", out.width="50%"}
plot(density(subswiss[,3], kernel = "gaussian", bw = 0.1))
plot(density(subswiss[,3], kernel = "gaussian", bw = 0.3))
plot(density(subswiss[,3], kernel = "gaussian", bw = 0.5))
plot(density(subswiss[,3], kernel = "gaussian", bw = 3))
```

We can find appropriate bandwidths using the dpik function

```{r}
bw <- c(dpik(subswiss$top_margin), dpik(subswiss$diag_length))
bw

```

```{r, figures-side3, fig.show="hold", out.width="50%"}
plot(density(subswiss$top_margin, kernel = "gaussian", bw = bw[1]))
plot(density(subswiss$diag_length, kernel = "gaussian", bw = bw[2]))
```

####  b. Using the bivariate Gaussian kernel, estimate the bivariate density of the two variables using (1) a contour plot and (2) a perspective plot. Use the bandwidths chosen in part (a).



***\newline\newline Contour plot***

```{r}
a=subswiss[,c(2,3)]
```

```{r, warning=FALSE, fig.height=4}
density <- bkde2D(a, bandwidth = bw)
plot(a,kernel = "gaussian", 
     xlab = "Diagonal Length", 
     ylab = "Top Margin", 
     main = "Contour Plot")

contour(x = density$x1, y = density$x2, z = density$fhat, add = TRUE)

```


***Perspective plot***


```{r, fig.height=4}
persp(x = density$x1, y = density$x2, z = density$fhat, phi=30, xlab = "Diagonal Length",
      ylab = "Top Margin",
      main = "Perspective Plot",
      zlab = "density"
)

```



#### c. Plot the scatterplot, highlighting the points with different colors according to whether the bills are real or fake. Explain your findings.

***\newline\newline Scatterplot***

```{r, fig.height=4, fig.align = "center"}
library(ggplot2)
ggplot(subswiss, aes(x=top_margin, y=diag_length, color=note)) + 
  geom_point(size=2) 
```

***Scatterplot matrix with density information***

```{r, fig.height=4, fig.align = "center"}
kdepairs(subswiss[,c("top_margin", "diag_length")])
```

### Problem 9: Examine the multivariate normality of the BCG data from the HSAUR2 package

```{r}
library(HSAUR2)
```

```{r}
data(BCG, package = "HSAUR2")
BCG
```

#### excluding the year and the study number. Document your process as follows.

#### a. Find the column-means vector \newline \newline

First we will drop year and study number columns from BCG data.

```{r}
BCG <- BCG[,-c(1,7) ]
BCG
```

***Column means vector***
  
  ```{r}
BCGbar <- colMeans(BCG)
BCGbar
```

#### b. Find the covariance matrix \newline \newline

***Covariance matrix***
  
  ```{r}
BCGcov <- cov(BCG)
BCGcov
```

#### c. Find the Mahalanobis distances using the data and the values determined in parts (a) and (b) \newline \newline

***Mahalanobis distances***

```{r}
d2 <- mahalanobis(BCG, BCGbar, BCGcov)
d2
```

#### d. Sort the Mahalanobis distances from smallest to largest \newline \newline

***Sorted Mahalanobis distances***

```{r}
sort_d2=sort(d2, decreasing = F)
sort_d2
```

#### \newline\newline \newline \newline\newline e. Plot the sorte d Mahalanobis distances vs the chi-square (of the appropriate number of degrees of freedom) quantiles. \newline \newline

***Mahalanobis distances vs the chi-square Plot***

```{r, fig.height=4, fig.align = "center"}
quantiles <- qchisq((1:nrow(BCG) - 1/2)/nrow(BCG), df = ncol(BCG))
plot(quantiles, sort_d2,
xlab = expression(paste(chi[3]^2, " Quantile")),
ylab = "Ordered squared distances")
abline(a = 0, b = 1)
```

#### f. Interpret the data. Is it multivariate normal? \newline \newline

For data to be multivariate normal, its chi-squared distribution should lead to a straight line through the origin, and there should not be outliers present in distribution.
As we can see in (e), the straight line has not originated from origin and outliers are also seen in distribution so the given data is approximately not a multivariate normal.

